{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本次作業將先訓練判斷 Fashion MNIST 資料集之模型，再將該模型部分神經網路拿來訓練 CIFAR-10 資料集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 匯入所需套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Layers for FNN\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "# Layers for CNN\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "# For data preprocessing\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 讀入並整理 Fashion MNSIT 資料集\n",
    "* Fashion MNSIT 是包含 10 種類的灰階小圖資料集，每張圖的尺寸為  28×28 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fashion_mnist\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train0, y_train0), (x_test0, y_test0) = fashion_mnist.load_data()\n",
    "\n",
    "# 1 stands for one channel(greyscale)\n",
    "# Normalize the range of features\n",
    "x_train = x_train0.reshape(60000, 28, 28, 1) / x_train0.max()\n",
    "x_test = x_test0.reshape(10000, 28, 28, 1) / x_test0.max()\n",
    "\n",
    "# One-hot encoding\n",
    "y_train = to_categorical(y_train0, 10)\n",
    "y_test = to_categorical(y_test0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢視測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(x_train0.shape[0])\n",
    "x_sample = x_train0[idx]\n",
    "y_sample = y_train0[idx].squeeze()\n",
    "\n",
    "plt.imshow(x_sample)\n",
    "plt.title(name_list[y_sample])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_test.shape, sep=\"\\n\") # 查看數據型態"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 建立用於分類 Fashion MNSIT 的卷積神經網路"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_layers = [Conv2D(24, (3, 3), padding='same', input_shape=(28, 28, 1), activation='relu', name='Conv_1'),\n",
    "              MaxPooling2D(pool_size = (2,2)),\n",
    "              Conv2D(96, (3, 3), padding='same', activation='relu', name='Conv_2'),\n",
    "              MaxPooling2D(pool_size = (2,2)),\n",
    "              Conv2D(384, (3, 3), padding='same', activation='relu', name='Conv_3'),\n",
    "              MaxPooling2D(pool_size = (2,2))]\n",
    "\n",
    "FC_layers = [Dropout(0.25),\n",
    "             Flatten(),\n",
    "             Dense(units=128, activation='relu'),\n",
    "             Dense(units=10, activation='softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 28, 28, 24)        240       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 24)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 14, 14, 96)        20832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 96)          0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 7, 7, 384)         332160    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 3, 384)         0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 384)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               442496    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 797,018\n",
      "Trainable params: 797,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(CNN_layers + FC_layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#組裝神經網路\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=Adam(),\n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "600/600 [==============================] - 167s 278ms/step - loss: 0.4839 - categorical_accuracy: 0.8229 - val_loss: 0.3264 - val_categorical_accuracy: 0.8842\n",
      "Epoch 2/5\n",
      "600/600 [==============================] - 170s 283ms/step - loss: 0.2940 - categorical_accuracy: 0.8921 - val_loss: 0.2741 - val_categorical_accuracy: 0.8992\n",
      "Epoch 3/5\n",
      "600/600 [==============================] - 171s 284ms/step - loss: 0.2507 - categorical_accuracy: 0.9084 - val_loss: 0.2476 - val_categorical_accuracy: 0.9092\n",
      "Epoch 4/5\n",
      "600/600 [==============================] - 170s 284ms/step - loss: 0.2187 - categorical_accuracy: 0.9203 - val_loss: 0.2562 - val_categorical_accuracy: 0.9057\n",
      "Epoch 5/5\n",
      "600/600 [==============================] - 175s 292ms/step - loss: 0.1981 - categorical_accuracy: 0.9269 - val_loss: 0.2328 - val_categorical_accuracy: 0.9146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x251d06af940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, \n",
    "          batch_size=100, \n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型預測: 預測資料集的準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 39s 21ms/step - loss: 0.1619 - categorical_accuracy: 0.9402\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2328 - categorical_accuracy: 0.9146\n",
      "Train Accuracy: 94.01666522026062\n",
      "Test Accuracy: 91.46000146865845\n"
     ]
    }
   ],
   "source": [
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_test = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 遷移學習(Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 讀入整理 CIFAR 10 資料集\n",
    "* CIFAR 10 是包含 10 種類的彩色小圖資料集，每張圖的尺寸為  32×32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR 10\n",
    "(u_train, v_train0), (u_test, v_test0) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize the range of features\n",
    "u_train = u_train / u_train.max()\n",
    "u_test = u_test / u_test.max()\n",
    "\n",
    "# One-hot encoding\n",
    "v_train = to_categorical(v_train0, 10)\n",
    "v_test = to_categorical(v_test0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢視測試資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAARj0lEQVR4nO3dzW8l6VXH8fNU3Rdf2912uxslM5kAmyySP4BIKAsWSEhEo2THmxS2gGDBH0BEkNixQIqE+AsQQhELEDskhBAIVixCAMEiPZmZNP1i97j9cq+vb9XDontEFvX7NS7c7tPN9yNl42fq3nLde1ydc+o5p9RaA0A+zes+AQDDCE4gKYITSIrgBJIiOIGkCE4gKYITSIrgfAOUUu6XUpallNNSytNSyl+VUj7/us8LrxbB+eZ4v9a6GxHvRMTDiPj2az4fvGIE5xum1rqKiO9ExJciIkopXy2l/HMp5Vkp5cNSyu/+6H9fSvlGKeWDUsphKeV3XtyFf/Y1nDquiOB8w5RStiPiFyLiH1/86CwivhER+xHx1Yj49VLK11/8t1+KiD+KiF+J53fcvYj43E2fM8YpPFubXynlfkTci4hNROxGxKOI+Lla63cH/ts/jIhaa/3tUso3I+KLtdZferG2HRGfRMTP11r/+qbOH+Nw53xzfL3Wuh8R84j4zYj421LKZ0spXy6l/E0p5XEp5Tgifi2eB3JExLsR8eGnL1BrPY+Iw5s+cYxDcL5haq1drfXPI6KLiK9ExJ9ExF9ExOdrrXsR8ccRUV785w8i4r1Pjy2lLCLi7s2eMcYiON8w5bmvRcSdiPi3iLgVEUe11lUp5aci4pd/5D//TkS8X0r56VLKLCK+Ff8TuEiO4Hxz/GUp5TQinkXE70fEr9ZavxcRvxERv1dKOYmIb0bEn316wIv134qIP43nd9GTeP7/Vy9u+NwxAgmh/0dKKbvxPCH0hVrr91/3+cDjzvmWK6W8X0rZLqXsRMQfRMR3I+L+6z0r/G8QnG+/r0XED1/87wsR8YuVfy69EfhnLZAUd04gqYlb/Pt/+p68rbo7bt/3Vz6RruoMf2/Wmkb/fSnyMFdN0Ofelo1+r3D/AtHvV/RJjnyvcf8SUucx5vxehbHft94dV/Vx1/4vSvNeP/OVLw9eZO6cQFIEJ5AUwQkkRXACSRGcQFIEJ5CULaW4dLJbG1ceMGtmsa+dXhyRDnfn0RRzPUwJ4/qrEddfShlT/noVxnx33HexaUwZK1q5ZsszY67ViO8Ad04gKYITSIrgBJIiOIGkCE4gKYITSMqWUjYbvQvjulWTQne7B/r+eksY7phq8+E3ty+27035aOR5ZNnXq3YZuRKL3bHSmZ0n5jxcuWRUucd8TxXunEBSBCeQFMEJJEVwAkkRnEBSNls7NmM1Jpvl3utGHyo3SbVq+gvZl7zmTOj6YiXXtrbmo14ze6+gsdfQZmRH9hDqOpctVy949e8Od04gKYITSIrgBJIiOIGkCE4gKYITSOqV9BAaw0xcSPMnxLX2v/aKzshU/tjPxY21GCNLTyLbbWnk93vc70YpBXhrEJxAUgQnkBTBCSRFcAJJEZxAUqNLKW4Xw5i0fGdTzTeXlreFCLtj5ea43RSuzDJu58m4vkmjdm68Aq5EN7ZcMqpcNeIY7pxAUgQnkBTBCSRFcAJJEZxAUgQnkNToUopLDY8qfDQ5xgHYckmSkQXVTl12JQwz8sIed3VJLlVUN43cllKue3o4pRTgrUFwAkkRnEBSBCeQFMEJJEVwAkn5WSnlctyrjsk0d+bvRBn3N6SRuzDG7bTob3B3jFNaPXHczXNpmql7VXWUPsTVS8o1z5UZWZrpq2mGZq6VbTgnzsVt+hnTE4w7J5AUwQkkRXACSRGcQFIEJ5DUSyZb5+gDM5rsZTT2oebrfhh6JJMJ7UxasGlbuVaruFZuZIFc8X2OxjRjctPNnVp1ZttOm3aT28XPu43JDI+oOHDnBJIiOIGkCE4gKYITSIrgBJIiOIGkfA8h9zD6tXOpcjNiwBylWur7qQTjzuPauTECnV7sOl0emDb6OFlVMJejd+MY7DU25yFKN2N7EjW2omPKJW7ciChJVXOvq/ZJevE+Vz4CwI0gOIGkCE4gKYITSIrgBJIiOIGkbCnFVg7GteExrzdusrV7K5UO96d3/X1x7LUacxquvHFpFiemhCE/a3OMK6XYGoamSylmyrp5vWK+w+46uv5C6/Xwi85mukfTpSlxKdw5gaQITiApghNIiuAEkiI4gaQITiApX0oxDZCqmfzrnujXr2caMZkShnsv2dnfpOUbs3PDNoRyYxxGlAFa04yrM43XNp0eodF1pgGV2jVhdlP0breQqWHYCeFiJ1Erm7VFdBv93blcr+Vaa05jY15TmcRCL47YVsOdE0iK4ASSIjiBpAhOICmCE0iK4ASSsqWUzeVSH9j6Kswgt9PCbYFx8zrM2rOTk8Gf7+3t6fcy1ZK+Hznp21Bn35mJzN3mQq5drNxnpssibTsbXlAzVCLCXaxSdQlDTxw3TdlMSedyeS7XTo6O5NrWVO8i+eT4E7k2n88Hf7797ufkMbowpnHnBJIiOIGkCE4gKYITSIrgBJIiOIGkbD2kbU2qvNFP7VeRDh/TjOtlNhtd3jg7Gy6l7OyY3QPmLGczfblsAyrzu6mdIptL/XtNzad2ePZUrk1M063FYles6CJA27htHSu5VM3unjNR/hpTfomIOD95ItfWrb43Xa50uarU4VLK2emWPGYyFaUqgzsnkBTBCSRFcAJJEZxAUgQnkJTN1p6e6IeGXQZyOhvOTLmM23qts7+mZU5crHVWbXl+NvjzH3zwgTym6/R5vPPuZ+Ta2vSqcRngzeXw+y2X+gH2ttHvtVoN/87P6Qu5XA4fN5vpzHZ3qa/95lxnjd334OR0OFu7u3tLHrM0D753ZvPGZu0y8/qh+OX58EPxjx7q60G2FniLEJxAUgQnkBTBCSRFcAJJEZxAUraUcvTDf5Vrm83ww78REbf39oeP6fXD0KsLnYbue516P1vpssJaPA/9+PBYHnNwcEeuLZ7qB6zdSABXSrkUpZTejE5oTCnl2bEuf/WdLitMZ8MPbbet/pxXpoRRL3RJ5+DgQK7NF8Mlh67qjQBHT/XD7Vui309ExMz0EJqY8Q+tKgWZB/BXp6dyTeHOCSRFcAJJEZxAUgQnkBTBCSRFcAJJ2VLK+vSBPrDVOzT6i+F0+GbzTB4zM/1tjo91Wt5UHOKiG/7b48ZM7O68I9e2zLTmorPycXKi0+iN6D20bUoAjfmbujIjEuaupCPKIs2W2blhxjuUrW25tr7QZRHVD2hjPujJRF+r5Zku351Xs5tlo99vKkowza4ZumAmwSvcOYGkCE4gKYITSIrgBJIiOIGkCE4gqZdMtjY7LcwugbOz4dJHH/qYXTPqYPNMlyImE90C/9ZkuDnV7p278pgfv3NPrt3Z0fWSZ8d6p8vZerhpVURE7YZLB5OJ/rvZVJ2yr+dmN8uWLn1slsM7Xaahr+90oj+zxuzq6Mz36vBwuHnWTDSNe34eeq1OdAnjk2PdhMxpJsPfg+WFafJm4kW+z5WPAHAjCE4gKYITSIrgBJIiOIGkCE4gKVtKmYiUcUTE6sLMoFgON+sqZlL2xszPuFjqNPSq043BZtvD77fY1b9X7xp1tTtyqbY6nT/fuS3XTsQk59bMKNne0uf/TqvXjk/17p5n58NlgFWvP+emNSUd83nevadLWcs6XN74xOxMunVLTeWOWGzpUlA7dxPOtVY0Q7v3Gb1Ta2JKOgp3TiApghNIiuAEkiI4gaQITiApm61dmVEHTaOzYOrB9+MT00NoS2c7b+3tybXdLZ1BnW8PZ/H2DvTD7f2O/r3+81C3/f/+/ftyzbmzPzz+YeeWnuS8f1ePjJgX3QPpX/7uH+Ta/f96NPjzyUz359mI/kcREbf2dYZ6s68/z1ORXX1ixhl8+PChXNvtdfb9wGR5e9Pzp6vDFYIDMzZk0l49M8ydE0iK4ASSIjiBpAhOICmCE0iK4ASSsqWU83Pdrn5mxgW0on/MfKrLFI+Wum3+41Y/fD2/1Cnv2+3wr/f48LE8Zv1Yp+UfPdU9Z548OZRrm06n89+rww/nf2Q2Fmw/+Fiu9b3uIaS7HEXs/sRPDv788Kk+qpo+QUszzuDjf/8PuVbEA/Pq5xER686UPczoh9Lr8oypEsVsNvya7QP93enEaBCHOyeQFMEJJEVwAkkRnEBSBCeQFMEJJPWSHkJ6p8jJqd5hcrkZftm21aWUZytdOni61OMMoujzqB8Np7Ybk5Y3GwtistDnPzf9aBZTfZkfHInyjPp5+L+oW3N9jlvbegePOu68M5/zWpcpqigRPWeuvygFXV6acQbmvbY3eu3IfK/8IOrhxY8fDu/siYi4vad3QincOYGkCE4gKYITSIrgBJIiOIGkCE4gKVtKcdOaf/CRThtP58MNnPb2PiuPKaF3uSzP9I4Vl0ZXa40plzStm8isz+Oy1an+udnBoyZA9735vcQ07IiI5VN9jtWUnS7Fzo7VWu+o6czOk6kZ1dCaNfV7u+sxMbtjykK/1+39A30e5nulxpRsmdEPt01TM4U7J5AUwQkkRXACSRGcQFIEJ5AUwQkkZUspZxe65jDf1fM62snw7oeHZqfFx0/1rpSji6uXSyIiWlEzmZhyycTsIImpvh6qJBIRsZro15xOh9PyrqQzM6UIt52iFPOa4vzd9ZiY0sGO2dE0mZjzFztWpuY8XPOvPV3Filvb+vybxpSCxPnf2d+Xx0xmNtSGz+HKRwC4EQQnkBTBCSRFcAJJEZxAUjaF9NETPdl6bR6+Pri3Pfjz2b6e1rxb9UPZ58e618tqaXrLlOGsWml1ds+uTfRaO9F/53Z3de+exWK491AVfWoiIrbmwxneCJ/JLeaJ/+3t4c9MPeT9svfabvRx7jVVDyH34Ptmox/On7oM9VRnlF0GWK3V4jLK+r0U7pxAUgQnkBTBCSRFcAJJEZxAUgQnkJQtpbz73ntyrbQmVT4bLg+0M/0U8nxfP0i/ODqSaxcXF3JNPTzu0uRunIF9CNw83H7vnm7Fv9gWYxzcZGVTtul6XVao5qF41f/GPdDfmLEKjXkv10Oo64ZLKernEb6UUsx5TM1nVt1oa7FUXC8j851TuHMCSRGcQFIEJ5AUwQkkRXACSRGcQFK2lHL33o/pA0W5JCKiEzE/NWMJFlWnyrf3hndMREScnJjpxG6kgTqPLf17TVt9/pNWX8q927oV/2x29d0KppISa1NacvWZhZjM7UoAvSk3uFKELysMr7mSjt1BYt6pNX2COvPdUa9ZTN8nNzFd4c4JJEVwAkkRnEBSBCeQFMEJJEVwAknZUspkpndotFM3rXk4Rd26sQTmPGZT00hqoc+jilENLs2/o3aJRMTOVDcom5idFlvmOuqp16ZMIZpgRUREo3douHLEVDRDc/piztH82Tf7PaJth1drHVGLiJeUe0aUdCJM6cY0UHPXSuHOCSRFcAJJEZxAUgQnkBTBCSRFcAJJ2VJK7xo4mRkUakZJNKZBlklDL8wEZdcsanMp5qiYFLrblbKY6LKN2+HgGkmZvRZyxSXlXUnH7d5oxJrd8WHKFBtTOnAFDFXucTuMXIlIldNexu0wUXrzybhmaPoYACkRnEBSBCeQFMEJJEVwAkkRnEBSfleK2XkymZp9JO1w06rGNMFqG53ydrtI2tBj57tmePeGe73ZRJdtih3pbnZ1mFS//K1N5t1t0Lj63odPX/Pqqf6RG0VGsbNLRhpXZNHX2DUFc7tjFO6cQFIEJ5AUwQkkRXACSRGcQFI2W9u4B7Zd5nIyvFbd3wKT+muqzgy35gH8IjLA9iFq082ounb75sF910+nE4e5RKjLkrq2/26vgjrOnodZG502FlwyecQQ6lfCj6e4em6YOyeQFMEJJEVwAkkRnEBSBCeQFMEJJGVLKS6R3pu4bkSu3/VYsY3xi3tg3pyH6GPTFTO12IwlsA8vm1R553rLiBqBLQG4adPmQqr3itDlmZssl9i3sk/ZmxKGO+qaH9x3D+f3lFKAtwfBCSRFcAJJEZxAUgQnkBTBCSRVXkVvFgD/d9w5gaQITiApghNIiuAEkiI4gaQITiCp/wZ5ABA1JKssowAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(u_train.shape[0])\n",
    "u_sample = u_train[idx]\n",
    "v_sample = v_train0[idx].squeeze()\n",
    "\n",
    "plt.imshow(u_sample)\n",
    "plt.title(name_list[v_sample])\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Layer transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New CNN layers for CIFAR-10\n",
    "CNN_layers_CF10 = [Conv2D(24, (3, 3), padding='same', input_shape=(32, 32, 3), activation='relu', name='Conv_1'),\n",
    "                   MaxPooling2D(pool_size = (2,2)),\n",
    "                   Conv2D(96, (3, 3), padding='same', activation='relu', name='Conv_2'),\n",
    "                   MaxPooling2D(pool_size = (2,2)),\n",
    "                   Conv2D(384, (3, 3), padding='same', activation='relu', name='Conv_3'),\n",
    "                   MaxPooling2D(pool_size = (2,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 32, 32, 24)        672       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 16, 24)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 16, 16, 96)        20832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 8, 8, 384)         332160    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               786560    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,141,514\n",
      "Trainable params: 353,664\n",
      "Non-trainable params: 787,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CF10 = Sequential(CNN_layers_CF10 + FC_layers)\n",
    "model_CF10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 採Frozen的訓練方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in FC_layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv2D)              (None, 32, 32, 24)        672       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 16, 16, 24)        0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv2D)              (None, 16, 16, 96)        20832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 8, 8, 96)          0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv2D)              (None, 8, 8, 384)         332160    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               786560    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,141,514\n",
      "Trainable params: 353,664\n",
      "Non-trainable params: 787,850\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CF10.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 174s 349ms/step - loss: 1.6863 - categorical_accuracy: 0.3899 - val_loss: 1.3447 - val_categorical_accuracy: 0.5241\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 197s 393ms/step - loss: 1.3436 - categorical_accuracy: 0.5233 - val_loss: 1.1960 - val_categorical_accuracy: 0.5870\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 201s 403ms/step - loss: 1.1874 - categorical_accuracy: 0.5829 - val_loss: 1.0741 - val_categorical_accuracy: 0.6241\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 202s 405ms/step - loss: 1.0820 - categorical_accuracy: 0.6223 - val_loss: 0.9886 - val_categorical_accuracy: 0.6574\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 182s 364ms/step - loss: 1.0116 - categorical_accuracy: 0.6483 - val_loss: 0.9691 - val_categorical_accuracy: 0.6669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x251de239a90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_CF10.compile(loss='categorical_crossentropy', \n",
    "                    optimizer=Adam(),\n",
    "                    metrics=['categorical_accuracy'])\n",
    "\n",
    "model_CF10.fit(u_train, v_train,\n",
    "                batch_size=100, \n",
    "                epochs=5,\n",
    "                validation_data=(u_test, v_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 模型預測: 預測資料集的準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8748 - categorical_accuracy: 0.7009\n",
      "313/313 [==============================] - 10s 32ms/step - loss: 0.9691 - categorical_accuracy: 0.6669\n",
      "Train Accuracy: 70.08799910545349\n",
      "Test Accuracy: 66.68999791145325\n"
     ]
    }
   ],
   "source": [
    "score_train = model_CF10.evaluate(u_train, v_train)\n",
    "score_test = model_CF10.evaluate(u_test, v_test)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 查看舊模型的預測表現是否受到影響"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 36s 19ms/step - loss: 0.1619 - categorical_accuracy: 0.94020s - loss: 0.1620 - categorical_accuracy\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.2328 - categorical_accuracy: 0.9146\n",
      "Train Accuracy: 94.01666522026062\n",
      "Test Accuracy: 91.46000146865845\n"
     ]
    }
   ],
   "source": [
    "score_train = model.evaluate(x_train, y_train)\n",
    "score_test = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Train Accuracy: {score_train[1]*100}')\n",
    "print(f'Test Accuracy: {score_test[1]*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 可見舊模型之預測表現不受遷移學習後影響，原因是我們採用了Frozen而非Fine-tune之訓練方式，在訓練新模型中並沒有將借來的神經網路重新訓練"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
