{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from ipywidgets import interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = pd.read_excel('taipei_running_data_x.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = pd.read_excel('taipei_running_data_y.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data       土地移轉總面積(平方公尺)  工  住  商  其他  交易筆棟數_土地  交易筆棟數_建物  交易筆棟數_車位      移轉層次  \\\n",
      "0          0.039182  0  1  0   0  0.035714       0.1  0.166667  0.222222   \n",
      "1          0.031495  0  1  0   0  0.071429       0.1  0.083333  0.305556   \n",
      "2          0.027822  0  1  0   0  0.035714       0.1  0.083333  0.194444   \n",
      "3          0.038581  0  0  1   0  0.035714       0.1  0.000000  0.333333   \n",
      "4          0.019160  0  0  1   0  0.035714       0.1  0.000000  0.194444   \n",
      "...             ... .. .. ..  ..       ...       ...       ...       ...   \n",
      "8675       0.123999  0  1  0   0  0.035714       0.1  0.083333  0.138889   \n",
      "8676       0.035054  0  0  1   0  0.071429       0.1  0.083333  0.527778   \n",
      "8677       0.003803  0  0  1   0  0.035714       0.1  0.000000  0.416667   \n",
      "8678       0.105976  0  1  0   0  0.071429       0.1  0.083333  0.444444   \n",
      "8679       0.022752  0  0  1   0  0.071429       0.1  0.083333  0.416667   \n",
      "\n",
      "          總樓層數  ...  其他.1  坡道平面  坡道機械  塔式車位  空格   交易標的橫坐標   交易標的縱坐標        屋齡  \\\n",
      "0     0.354839  ...     0     1     0     0   0  0.579262  0.337162  0.076814   \n",
      "1     0.580645  ...     0     1     0     0   0  0.787888  0.552158  0.349705   \n",
      "2     0.161290  ...     0     0     1     0   0  0.955935  0.566542  0.154290   \n",
      "3     0.161290  ...     0     0     0     0   1  0.787764  0.575805  0.587993   \n",
      "4     0.193548  ...     0     0     0     0   1  0.385013  0.411777  0.312172   \n",
      "...        ...  ...   ...   ...   ...   ...  ..       ...       ...       ...   \n",
      "8675  0.064516  ...     0     1     0     0   0  0.257360  0.938373  0.455022   \n",
      "8676  0.387097  ...     0     0     0     1   0  0.201220  0.899203  0.118175   \n",
      "8677  0.419355  ...     0     0     0     0   1  0.491442  0.068974  0.133822   \n",
      "8678  0.290323  ...     0     1     0     0   0  0.834381  0.518110  0.151075   \n",
      "8679  0.741935  ...     0     1     0     0   0  0.385760  0.260271  0.018577   \n",
      "\n",
      "           lat       lon  \n",
      "0     0.337084  0.578431  \n",
      "1     0.551303  0.788577  \n",
      "2     0.565061  0.957067  \n",
      "3     0.574945  0.788577  \n",
      "4     0.412331  0.384167  \n",
      "...        ...       ...  \n",
      "8675  0.939232  0.258638  \n",
      "8676  0.900243  0.202169  \n",
      "8677  0.069242  0.489153  \n",
      "8678  0.517093  0.834989  \n",
      "8679  0.260850  0.384206  \n",
      "\n",
      "[8680 rows x 46 columns]\n",
      "y_data       單價(元/平方公尺)\n",
      "0         268677\n",
      "1         176252\n",
      "2          98789\n",
      "3         171451\n",
      "4         153983\n",
      "...          ...\n",
      "8675       88643\n",
      "8676      175131\n",
      "8677      175953\n",
      "8678      222855\n",
      "8679      268212\n",
      "\n",
      "[8680 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"x_data\",x_data)\n",
    "print(\"y_data\",y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train       土地移轉總面積(平方公尺)  工  住  商  其他  交易筆棟數_土地  交易筆棟數_建物  交易筆棟數_車位      移轉層次  \\\n",
      "252        0.037590  0  1  0   0  0.071429       0.1  0.000000  0.250000   \n",
      "1839       0.017292  0  0  1   0  0.035714       0.1  0.083333  0.555556   \n",
      "718        0.028911  0  0  1   0  0.071429       0.1  0.083333  0.222222   \n",
      "3667       0.039654  0  1  0   0  0.035714       0.1  0.166667  0.361111   \n",
      "2883       0.007167  0  0  0   1  0.035714       0.1  0.000000  0.555556   \n",
      "...             ... .. .. ..  ..       ...       ...       ...       ...   \n",
      "4373       0.024572  0  1  0   0  0.035714       0.1  0.083333  0.472222   \n",
      "7891       0.014171  0  1  0   0  0.107143       0.1  0.000000  0.305556   \n",
      "4859       0.018657  0  0  1   0  0.035714       0.1  0.000000  0.277778   \n",
      "3264       0.027319  0  1  0   0  0.035714       0.1  0.000000  0.277778   \n",
      "2732       0.017357  0  0  1   0  0.035714       0.1  0.000000  0.194444   \n",
      "\n",
      "          總樓層數  ...  其他.1  坡道平面  坡道機械  塔式車位  空格   交易標的橫坐標   交易標的縱坐標        屋齡  \\\n",
      "252   0.096774  ...     0     0     0     0   1  0.556793  0.113008  0.557031   \n",
      "1839  0.548387  ...     0     1     0     0   0  0.391050  0.446445  0.002033   \n",
      "718   0.290323  ...     0     1     0     0   0  0.260534  0.357653  0.395604   \n",
      "3667  0.225806  ...     0     1     0     0   0  0.253563  0.845234  0.002316   \n",
      "2883  0.516129  ...     0     0     0     0   1  0.630423  0.422747  0.120444   \n",
      "...        ...  ...   ...   ...   ...   ...  ..       ...       ...       ...   \n",
      "4373  0.322581  ...     0     1     0     0   0  0.428892  0.503105  0.549752   \n",
      "7891  0.387097  ...     0     0     0     0   1  0.012821  0.828004  0.146490   \n",
      "4859  0.322581  ...     0     0     0     0   1  0.381776  0.480286  0.675490   \n",
      "3264  0.096774  ...     0     0     0     0   1  0.956681  0.541550  0.386197   \n",
      "2732  0.258065  ...     0     0     0     0   1  0.480115  0.071355  0.317041   \n",
      "\n",
      "           lat       lon  \n",
      "252   0.113049  0.554818  \n",
      "1839  0.446973  0.390379  \n",
      "718   0.358611  0.259197  \n",
      "3667  0.846123  0.254410  \n",
      "2883  0.422476  0.630118  \n",
      "...        ...       ...  \n",
      "4373  0.503499  0.428566  \n",
      "7891  0.829616  0.012968  \n",
      "4859  0.480837  0.381245  \n",
      "3264  0.540071  0.957678  \n",
      "2732  0.071660  0.477819  \n",
      "\n",
      "[6510 rows x 46 columns]\n",
      "y_train       單價(元/平方公尺)\n",
      "252       105163\n",
      "1839      223151\n",
      "718       125183\n",
      "3667      198987\n",
      "2883      244144\n",
      "...          ...\n",
      "4373      152647\n",
      "7891      140647\n",
      "4859      133067\n",
      "3264      142426\n",
      "2732      157404\n",
      "\n",
      "[6510 rows x 1 columns]\n",
      "X_test       土地移轉總面積(平方公尺)  工  住  商  其他  交易筆棟數_土地  交易筆棟數_建物  交易筆棟數_車位      移轉層次  \\\n",
      "2405       0.011929  0  0  1   0  0.035714       0.1  0.083333  0.222222   \n",
      "8454       0.073522  0  1  0   0  0.107143       0.1  0.000000  0.222222   \n",
      "6657       0.058733  0  1  0   0  0.035714       0.1  0.083333  0.250000   \n",
      "8037       0.017292  0  0  1   0  0.035714       0.1  0.083333  0.222222   \n",
      "8317       0.054897  0  1  0   0  0.071429       0.1  0.166667  0.472222   \n",
      "...             ... .. .. ..  ..       ...       ...       ...       ...   \n",
      "991        0.008337  0  0  1   0  0.035714       0.1  0.083333  0.166667   \n",
      "4824       0.049567  0  1  0   0  0.035714       0.2  0.083333  0.166667   \n",
      "8168       0.049161  0  1  0   0  0.035714       0.1  0.083333  0.361111   \n",
      "899        0.028554  0  0  1   0  0.035714       0.1  0.083333  0.222222   \n",
      "6209       0.067248  0  1  0   0  0.035714       0.1  0.083333  0.250000   \n",
      "\n",
      "          總樓層數  ...  其他.1  坡道平面  坡道機械  塔式車位  空格   交易標的橫坐標   交易標的縱坐標        屋齡  \\\n",
      "2405  0.419355  ...     0     0     0     0   0  0.573100  0.279934  0.028173   \n",
      "8454  0.064516  ...     0     0     0     0   1  0.493372  0.257115  0.858048   \n",
      "6657  0.290323  ...     0     1     0     0   0  0.811726  0.616423  0.028220   \n",
      "8037  0.387097  ...     0     1     0     0   0  0.657434  0.389631  0.174569   \n",
      "8317  0.419355  ...     0     1     0     0   0  0.642870  0.435424  0.103522   \n",
      "...        ...  ...   ...   ...   ...   ...  ..       ...       ...       ...   \n",
      "991   0.387097  ...     0     0     1     0   0  0.371071  0.454931  0.248357   \n",
      "4824  0.258065  ...     0     0     0     0   0  0.455966  0.765497  0.299267   \n",
      "8168  0.290323  ...     0     1     0     0   0  0.254061  0.896099  0.011676   \n",
      "899   0.709677  ...     0     1     0     0   0  0.654011  0.054745  0.030820   \n",
      "6209  0.387097  ...     0     1     0     0   0  0.251883  0.283763  0.061073   \n",
      "\n",
      "           lat       lon  \n",
      "2405  0.279888  0.571974  \n",
      "8454  0.257342  0.491993  \n",
      "6657  0.615469  0.812807  \n",
      "8037  0.389272  0.657015  \n",
      "8317  0.435108  0.642654  \n",
      "...        ...       ...  \n",
      "991   0.455522  0.370399  \n",
      "4824  0.765751  0.456955  \n",
      "8168  0.896975  0.255139  \n",
      "899   0.054462  0.651893  \n",
      "6209  0.284761  0.250197  \n",
      "\n",
      "[2170 rows x 46 columns]\n",
      "y_test       單價(元/平方公尺)\n",
      "2405      241861\n",
      "8454      196381\n",
      "6657      160366\n",
      "8037      192215\n",
      "8317      246117\n",
      "...          ...\n",
      "991       250170\n",
      "4824      162378\n",
      "8168      216122\n",
      "899       133692\n",
      "6209      156446\n",
      "\n",
      "[2170 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train\",X_train)\n",
    "print(\"y_train\",y_train)\n",
    "print(\"X_test\",X_test)\n",
    "print(\"y_test\",y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# For functional API\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(LSTM(50,dropout=0.2, input_shape(), return_sequence = True))\n",
    "# model.add(LSTM(50,dropout=0.2, return_sequence = True))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(256,input_dim=46,activation='relu'))\n",
    "model.add(Dense(256,input_dim=46,activation=tf.keras.layers.LeakyReLU(alpha=0.2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Dense(128,activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=tf.keras.layers.LeakyReLU(alpha=0.2)))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Dense(64,activation='relu'))\n",
    "model.add(Dense(64,activation=tf.keras.layers.LeakyReLU(alpha=0.2)))\n",
    "model.add(Dense(1,activation=tf.keras.layers.LeakyReLU(alpha=0.2)))\n",
    "model.compile(loss='mean_absolute_percentage_error',optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 256)               12032     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 54,273\n",
      "Trainable params: 53,761\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor = 'loss', patience = 30, verbose = 0, mode = 'auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 99.9898 - val_loss: 99.9930\n",
      "Epoch 2/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 99.8355 - val_loss: 99.8876\n",
      "Epoch 3/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 98.8923 - val_loss: 99.2299\n",
      "Epoch 4/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 95.9232 - val_loss: 96.7020\n",
      "Epoch 5/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 88.9386 - val_loss: 90.6496\n",
      "Epoch 6/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 75.8923 - val_loss: 77.2829\n",
      "Epoch 7/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 58.7956 - val_loss: 60.8241\n",
      "Epoch 8/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 43.5117 - val_loss: 45.9354\n",
      "Epoch 9/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 29.6863 - val_loss: 30.8302\n",
      "Epoch 10/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 24.7533 - val_loss: 24.7907\n",
      "Epoch 11/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 23.8585 - val_loss: 24.1593\n",
      "Epoch 12/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 24.0777 - val_loss: 24.5916\n",
      "Epoch 13/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 24.0813 - val_loss: 24.7171\n",
      "Epoch 14/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.5199 - val_loss: 25.7917\n",
      "Epoch 15/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.1476 - val_loss: 24.3877\n",
      "Epoch 16/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.5508 - val_loss: 24.9391\n",
      "Epoch 17/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.0565 - val_loss: 23.9113\n",
      "Epoch 18/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.1528 - val_loss: 23.9756\n",
      "Epoch 19/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.0373 - val_loss: 24.5236\n",
      "Epoch 20/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.9812 - val_loss: 23.7235\n",
      "Epoch 21/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.7162 - val_loss: 23.3060\n",
      "Epoch 22/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.9900 - val_loss: 23.4152\n",
      "Epoch 23/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.7807 - val_loss: 23.3547\n",
      "Epoch 24/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 23.0229 - val_loss: 23.1300\n",
      "Epoch 25/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.5355 - val_loss: 23.0105\n",
      "Epoch 26/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.9056 - val_loss: 22.9307\n",
      "Epoch 27/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.2762 - val_loss: 23.1326\n",
      "Epoch 28/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.7081 - val_loss: 22.6710\n",
      "Epoch 29/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.6537 - val_loss: 22.7829\n",
      "Epoch 30/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.3929 - val_loss: 22.4650\n",
      "Epoch 31/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.0616 - val_loss: 22.4916\n",
      "Epoch 32/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.6194 - val_loss: 22.5473\n",
      "Epoch 33/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.0563 - val_loss: 22.3526\n",
      "Epoch 34/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.9002 - val_loss: 22.4365\n",
      "Epoch 35/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.6390 - val_loss: 22.5011\n",
      "Epoch 36/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.6621 - val_loss: 22.1750\n",
      "Epoch 37/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 22.0038 - val_loss: 22.1357\n",
      "Epoch 38/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.7387 - val_loss: 22.0469\n",
      "Epoch 39/500\n",
      "36/36 [==============================] - ETA: 0s - loss: 21.81 - 0s 5ms/step - loss: 21.7749 - val_loss: 22.1041\n",
      "Epoch 40/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.7658 - val_loss: 22.0337\n",
      "Epoch 41/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.7361 - val_loss: 21.8978\n",
      "Epoch 42/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1728 - val_loss: 22.3155\n",
      "Epoch 43/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.4674 - val_loss: 21.8676\n",
      "Epoch 44/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1933 - val_loss: 21.8056\n",
      "Epoch 45/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.2551 - val_loss: 21.7780\n",
      "Epoch 46/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.6891 - val_loss: 21.6920\n",
      "Epoch 47/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1714 - val_loss: 21.8288\n",
      "Epoch 48/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.5925 - val_loss: 21.7424\n",
      "Epoch 49/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1683 - val_loss: 21.8758\n",
      "Epoch 50/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 21.1762 - val_loss: 21.4260\n",
      "Epoch 51/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1417 - val_loss: 21.6053\n",
      "Epoch 52/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1864 - val_loss: 21.4799\n",
      "Epoch 53/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.1623 - val_loss: 21.3945\n",
      "Epoch 54/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.2881 - val_loss: 21.1976\n",
      "Epoch 55/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 21.0290 - val_loss: 21.2613\n",
      "Epoch 56/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.9179 - val_loss: 21.4148\n",
      "Epoch 57/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.3848 - val_loss: 21.2392\n",
      "Epoch 58/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.3874 - val_loss: 21.3076\n",
      "Epoch 59/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.0653 - val_loss: 20.9947\n",
      "Epoch 60/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.7333 - val_loss: 21.1295\n",
      "Epoch 61/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.2537 - val_loss: 20.8860\n",
      "Epoch 62/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.5677 - val_loss: 21.4066\n",
      "Epoch 63/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.5955 - val_loss: 20.9353\n",
      "Epoch 64/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.8900 - val_loss: 20.8756\n",
      "Epoch 65/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.5980 - val_loss: 20.6890\n",
      "Epoch 66/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 20.9311 - val_loss: 20.6840\n",
      "Epoch 67/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.5723 - val_loss: 20.6747\n",
      "Epoch 68/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.9175 - val_loss: 20.7884\n",
      "Epoch 69/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.7550 - val_loss: 20.6602\n",
      "Epoch 70/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.5972 - val_loss: 20.5809\n",
      "Epoch 71/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 21.3595 - val_loss: 20.5839\n",
      "Epoch 72/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.5612 - val_loss: 20.5489\n",
      "Epoch 73/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 21.5210 - val_loss: 20.3635\n",
      "Epoch 74/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.2164 - val_loss: 20.3031\n",
      "Epoch 75/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.4222 - val_loss: 20.3836\n",
      "Epoch 76/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.6128 - val_loss: 20.4479\n",
      "Epoch 77/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.4326 - val_loss: 20.3383\n",
      "Epoch 78/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.4358 - val_loss: 20.2092\n",
      "Epoch 79/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.6528 - val_loss: 20.2246\n",
      "Epoch 80/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.4188 - val_loss: 20.1657\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 4ms/step - loss: 20.0127 - val_loss: 20.2634\n",
      "Epoch 82/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.7934 - val_loss: 20.0509\n",
      "Epoch 83/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.9250 - val_loss: 20.8291\n",
      "Epoch 84/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.0680 - val_loss: 19.9763\n",
      "Epoch 85/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 20.1525 - val_loss: 20.1225\n",
      "Epoch 86/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.0416 - val_loss: 20.0403\n",
      "Epoch 87/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.0735 - val_loss: 20.5883\n",
      "Epoch 88/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.3070 - val_loss: 19.7812\n",
      "Epoch 89/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.8485 - val_loss: 19.8291\n",
      "Epoch 90/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.9523 - val_loss: 19.6865\n",
      "Epoch 91/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.6160 - val_loss: 19.6684\n",
      "Epoch 92/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.3533 - val_loss: 19.7450\n",
      "Epoch 93/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.3820 - val_loss: 19.7915\n",
      "Epoch 94/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 20.2956 - val_loss: 20.0195\n",
      "Epoch 95/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.7165 - val_loss: 19.5856\n",
      "Epoch 96/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.6384 - val_loss: 19.4739\n",
      "Epoch 97/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.6466 - val_loss: 19.7724\n",
      "Epoch 98/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.3762 - val_loss: 19.7860\n",
      "Epoch 99/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4511 - val_loss: 19.4432\n",
      "Epoch 100/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.5570 - val_loss: 19.4140\n",
      "Epoch 101/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.5271 - val_loss: 19.5550\n",
      "Epoch 102/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.2875 - val_loss: 19.3575\n",
      "Epoch 103/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.8542 - val_loss: 19.5280\n",
      "Epoch 104/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.1672 - val_loss: 19.5234\n",
      "Epoch 105/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.5775 - val_loss: 19.5647\n",
      "Epoch 106/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4828 - val_loss: 19.3670\n",
      "Epoch 107/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.7637 - val_loss: 19.4442\n",
      "Epoch 108/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.8912 - val_loss: 19.2417\n",
      "Epoch 109/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.2750 - val_loss: 19.3729\n",
      "Epoch 110/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.3576 - val_loss: 19.5929\n",
      "Epoch 111/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.5115 - val_loss: 19.3642\n",
      "Epoch 112/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9969 - val_loss: 19.1329\n",
      "Epoch 113/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.7444 - val_loss: 19.1617\n",
      "Epoch 114/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.5705 - val_loss: 19.2870\n",
      "Epoch 115/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9855 - val_loss: 19.1070\n",
      "Epoch 116/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9298 - val_loss: 19.1122\n",
      "Epoch 117/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.3334 - val_loss: 19.2233\n",
      "Epoch 118/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.7356 - val_loss: 18.9142\n",
      "Epoch 119/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.1159 - val_loss: 18.9251\n",
      "Epoch 120/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.0173 - val_loss: 18.9703\n",
      "Epoch 121/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9344 - val_loss: 18.9994\n",
      "Epoch 122/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.6528 - val_loss: 18.9974\n",
      "Epoch 123/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.5456 - val_loss: 19.2261\n",
      "Epoch 124/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.0029 - val_loss: 18.8648\n",
      "Epoch 125/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9200 - val_loss: 19.0004\n",
      "Epoch 126/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4228 - val_loss: 19.0364\n",
      "Epoch 127/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.3224 - val_loss: 19.2038\n",
      "Epoch 128/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4117 - val_loss: 20.0124\n",
      "Epoch 129/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9397 - val_loss: 18.8948\n",
      "Epoch 130/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.3246 - val_loss: 18.8553\n",
      "Epoch 131/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.4107 - val_loss: 18.9014\n",
      "Epoch 132/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4980 - val_loss: 18.6774\n",
      "Epoch 133/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.8322 - val_loss: 18.8361\n",
      "Epoch 134/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.9760 - val_loss: 18.8059\n",
      "Epoch 135/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.0576 - val_loss: 19.1489\n",
      "Epoch 136/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.8464 - val_loss: 18.8424\n",
      "Epoch 137/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.0347 - val_loss: 19.0258\n",
      "Epoch 138/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6039 - val_loss: 19.4866\n",
      "Epoch 139/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6755 - val_loss: 19.2723\n",
      "Epoch 140/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.7356 - val_loss: 18.7463\n",
      "Epoch 141/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.5112 - val_loss: 18.6874\n",
      "Epoch 142/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3389 - val_loss: 18.6634\n",
      "Epoch 143/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.8259 - val_loss: 18.5510\n",
      "Epoch 144/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 19.6196 - val_loss: 19.0953\n",
      "Epoch 145/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.0976 - val_loss: 18.5768\n",
      "Epoch 146/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.7736 - val_loss: 18.8121\n",
      "Epoch 147/500\n",
      "36/36 [==============================] - ETA: 0s - loss: 18.54 - 0s 5ms/step - loss: 18.6324 - val_loss: 18.5367\n",
      "Epoch 148/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.9680 - val_loss: 18.7025\n",
      "Epoch 149/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.6290 - val_loss: 18.6249\n",
      "Epoch 150/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2026 - val_loss: 18.7024\n",
      "Epoch 151/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.8336 - val_loss: 18.9762\n",
      "Epoch 152/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 19.0918 - val_loss: 18.3221\n",
      "Epoch 153/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3931 - val_loss: 18.4842\n",
      "Epoch 154/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.8784 - val_loss: 18.7774\n",
      "Epoch 155/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2023 - val_loss: 18.6976\n",
      "Epoch 156/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3844 - val_loss: 18.6537\n",
      "Epoch 157/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.6320 - val_loss: 18.4603\n",
      "Epoch 158/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.5980 - val_loss: 18.5043\n",
      "Epoch 159/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3681 - val_loss: 18.6372\n",
      "Epoch 160/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3435 - val_loss: 19.5564\n",
      "Epoch 161/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.7900 - val_loss: 18.2718\n",
      "Epoch 162/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4531 - val_loss: 18.7891\n",
      "Epoch 163/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2702 - val_loss: 18.6409\n",
      "Epoch 164/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3844 - val_loss: 18.3112\n",
      "Epoch 165/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2675 - val_loss: 18.3500\n",
      "Epoch 166/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4278 - val_loss: 18.1470\n",
      "Epoch 167/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6251 - val_loss: 18.4306\n",
      "Epoch 168/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7307 - val_loss: 18.3639\n",
      "Epoch 169/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3478 - val_loss: 18.4102\n",
      "Epoch 170/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 18.1153 - val_loss: 18.4463\n",
      "Epoch 171/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.1232 - val_loss: 18.4150\n",
      "Epoch 172/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.0011 - val_loss: 18.7826\n",
      "Epoch 173/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4754 - val_loss: 18.4449\n",
      "Epoch 174/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.4126 - val_loss: 18.5258\n",
      "Epoch 175/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3047 - val_loss: 18.1612\n",
      "Epoch 176/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2080 - val_loss: 18.9700\n",
      "Epoch 177/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.0424 - val_loss: 18.3595\n",
      "Epoch 178/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.6459 - val_loss: 18.4944\n",
      "Epoch 179/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8874 - val_loss: 18.1912\n",
      "Epoch 180/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.1661 - val_loss: 18.2096\n",
      "Epoch 181/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.1543 - val_loss: 18.2526\n",
      "Epoch 182/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.7577 - val_loss: 18.2555\n",
      "Epoch 183/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.0229 - val_loss: 18.3804\n",
      "Epoch 184/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.6190 - val_loss: 18.0984\n",
      "Epoch 185/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7701 - val_loss: 18.2430\n",
      "Epoch 186/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7852 - val_loss: 18.3801\n",
      "Epoch 187/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.3220 - val_loss: 18.7041\n",
      "Epoch 188/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.6534 - val_loss: 18.0889\n",
      "Epoch 189/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4935 - val_loss: 18.2182\n",
      "Epoch 190/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.9687 - val_loss: 18.1086\n",
      "Epoch 191/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7902 - val_loss: 18.0275\n",
      "Epoch 192/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.2150 - val_loss: 18.1681\n",
      "Epoch 193/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8871 - val_loss: 18.0173\n",
      "Epoch 194/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.9334 - val_loss: 18.0781\n",
      "Epoch 195/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.4441 - val_loss: 18.0865\n",
      "Epoch 196/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.8400 - val_loss: 18.0916\n",
      "Epoch 197/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.9029 - val_loss: 18.1410\n",
      "Epoch 198/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.8236 - val_loss: 18.1253\n",
      "Epoch 199/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 18.1975 - val_loss: 18.0576\n",
      "Epoch 200/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4618 - val_loss: 17.9728\n",
      "Epoch 201/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7977 - val_loss: 17.9750\n",
      "Epoch 202/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.7915 - val_loss: 18.1311\n",
      "Epoch 203/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.9103 - val_loss: 17.9994\n",
      "Epoch 204/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.2557 - val_loss: 17.9449\n",
      "Epoch 205/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4393 - val_loss: 18.2789\n",
      "Epoch 206/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4643 - val_loss: 18.0566\n",
      "Epoch 207/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.4855 - val_loss: 18.1733\n",
      "Epoch 208/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4441 - val_loss: 18.5117\n",
      "Epoch 209/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.7073 - val_loss: 18.0604\n",
      "Epoch 210/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.2371 - val_loss: 18.3210\n",
      "Epoch 211/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.3165 - val_loss: 17.8815\n",
      "Epoch 212/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4550 - val_loss: 17.9878\n",
      "Epoch 213/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.5392 - val_loss: 18.5612\n",
      "Epoch 214/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4311 - val_loss: 18.1798\n",
      "Epoch 215/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.1427 - val_loss: 18.1703\n",
      "Epoch 216/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.6452 - val_loss: 17.9997\n",
      "Epoch 217/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.2850 - val_loss: 17.8425\n",
      "Epoch 218/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.9740 - val_loss: 18.1405\n",
      "Epoch 219/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.7036 - val_loss: 18.1118\n",
      "Epoch 220/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 17.1000 - val_loss: 17.8178\n",
      "Epoch 221/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.1591 - val_loss: 18.0761\n",
      "Epoch 222/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4288 - val_loss: 17.9615\n",
      "Epoch 223/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.1459 - val_loss: 18.3058\n",
      "Epoch 224/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.4815 - val_loss: 17.9579\n",
      "Epoch 225/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.4488 - val_loss: 18.1032\n",
      "Epoch 226/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.0062 - val_loss: 18.4100\n",
      "Epoch 227/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.5080 - val_loss: 17.9609\n",
      "Epoch 228/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.2507 - val_loss: 17.9014\n",
      "Epoch 229/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.5455 - val_loss: 17.9041\n",
      "Epoch 230/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.8913 - val_loss: 17.9093\n",
      "Epoch 231/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.8076 - val_loss: 18.3545\n",
      "Epoch 232/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.9881 - val_loss: 17.9246\n",
      "Epoch 233/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.9584 - val_loss: 17.8492\n",
      "Epoch 234/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.1073 - val_loss: 18.0882\n",
      "Epoch 235/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0644 - val_loss: 17.8481\n",
      "Epoch 236/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0999 - val_loss: 17.8888\n",
      "Epoch 237/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 17.4480 - val_loss: 18.5381\n",
      "Epoch 238/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.3243 - val_loss: 18.1530\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0360 - val_loss: 18.1037\n",
      "Epoch 240/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0403 - val_loss: 18.0278\n",
      "Epoch 241/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.8470 - val_loss: 17.7940\n",
      "Epoch 242/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6745 - val_loss: 18.3387\n",
      "Epoch 243/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.8578 - val_loss: 18.1479\n",
      "Epoch 244/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.2617 - val_loss: 17.9962\n",
      "Epoch 245/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.9068 - val_loss: 18.0198\n",
      "Epoch 246/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7721 - val_loss: 18.3877\n",
      "Epoch 247/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.8858 - val_loss: 18.0984\n",
      "Epoch 248/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0900 - val_loss: 18.0043\n",
      "Epoch 249/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7093 - val_loss: 17.9902\n",
      "Epoch 250/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.5654 - val_loss: 17.8576\n",
      "Epoch 251/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0663 - val_loss: 17.9647\n",
      "Epoch 252/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7366 - val_loss: 17.7697\n",
      "Epoch 253/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7835 - val_loss: 17.8276\n",
      "Epoch 254/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6841 - val_loss: 17.8386\n",
      "Epoch 255/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6409 - val_loss: 17.8303\n",
      "Epoch 256/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0116 - val_loss: 18.0006\n",
      "Epoch 257/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.9174 - val_loss: 17.8430\n",
      "Epoch 258/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.5620 - val_loss: 18.0204\n",
      "Epoch 259/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6329 - val_loss: 17.7829\n",
      "Epoch 260/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.8755 - val_loss: 17.9960\n",
      "Epoch 261/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 17.0366 - val_loss: 17.6967\n",
      "Epoch 262/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.7372 - val_loss: 17.7104\n",
      "Epoch 263/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.6640 - val_loss: 17.7486\n",
      "Epoch 264/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.4283 - val_loss: 17.8722\n",
      "Epoch 265/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.5832 - val_loss: 17.8461\n",
      "Epoch 266/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.7278 - val_loss: 17.7356\n",
      "Epoch 267/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.6121 - val_loss: 17.8430\n",
      "Epoch 268/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.3772 - val_loss: 17.7268\n",
      "Epoch 269/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.5996 - val_loss: 17.8186\n",
      "Epoch 270/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4485 - val_loss: 17.8338\n",
      "Epoch 271/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4708 - val_loss: 18.3852\n",
      "Epoch 272/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.1846 - val_loss: 17.7669\n",
      "Epoch 273/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.0985 - val_loss: 17.6980\n",
      "Epoch 274/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4706 - val_loss: 17.7259\n",
      "Epoch 275/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.1820 - val_loss: 18.3920\n",
      "Epoch 276/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4223 - val_loss: 17.7514\n",
      "Epoch 277/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4099 - val_loss: 18.1307\n",
      "Epoch 278/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.2291 - val_loss: 18.0860\n",
      "Epoch 279/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.5286 - val_loss: 17.6387\n",
      "Epoch 280/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4161 - val_loss: 18.1148\n",
      "Epoch 281/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.4122 - val_loss: 17.8031\n",
      "Epoch 282/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.2370 - val_loss: 18.0154\n",
      "Epoch 283/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.2413 - val_loss: 17.4949\n",
      "Epoch 284/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.1196 - val_loss: 17.5556\n",
      "Epoch 285/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.9107 - val_loss: 17.7754\n",
      "Epoch 286/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 16.1551 - val_loss: 17.8158\n",
      "Epoch 287/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.9903 - val_loss: 17.6733\n",
      "Epoch 288/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.1371 - val_loss: 17.8514\n",
      "Epoch 289/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.0723 - val_loss: 18.0866\n",
      "Epoch 290/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.8765 - val_loss: 18.1479\n",
      "Epoch 291/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.9833 - val_loss: 17.7228\n",
      "Epoch 292/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.0968 - val_loss: 17.7050\n",
      "Epoch 293/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.9597 - val_loss: 17.9979\n",
      "Epoch 294/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 16.0995 - val_loss: 17.6351\n",
      "Epoch 295/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6593 - val_loss: 17.6629\n",
      "Epoch 296/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.9071 - val_loss: 17.7030\n",
      "Epoch 297/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.8190 - val_loss: 17.8147\n",
      "Epoch 298/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.5982 - val_loss: 17.5622\n",
      "Epoch 299/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.7489 - val_loss: 18.0121\n",
      "Epoch 300/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6721 - val_loss: 17.7676\n",
      "Epoch 301/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8651 - val_loss: 17.7522\n",
      "Epoch 302/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.7130 - val_loss: 17.5860\n",
      "Epoch 303/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.4611 - val_loss: 17.9682\n",
      "Epoch 304/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.8650 - val_loss: 17.8796\n",
      "Epoch 305/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.7057 - val_loss: 17.5138\n",
      "Epoch 306/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.5804 - val_loss: 17.6755\n",
      "Epoch 307/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.8806 - val_loss: 18.2293\n",
      "Epoch 308/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.4462 - val_loss: 18.3138\n",
      "Epoch 309/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6594 - val_loss: 17.6385\n",
      "Epoch 310/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6461 - val_loss: 17.5440\n",
      "Epoch 311/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6168 - val_loss: 17.6592\n",
      "Epoch 312/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5665 - val_loss: 17.5746\n",
      "Epoch 313/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.6806 - val_loss: 17.5563\n",
      "Epoch 314/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.5647 - val_loss: 17.4966\n",
      "Epoch 315/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5639 - val_loss: 17.8109\n",
      "Epoch 316/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.7110 - val_loss: 17.5856\n",
      "Epoch 317/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.7492 - val_loss: 17.6614\n",
      "Epoch 318/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5936 - val_loss: 17.8870\n",
      "Epoch 319/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5935 - val_loss: 17.7812\n",
      "Epoch 320/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4019 - val_loss: 17.5453\n",
      "Epoch 321/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5340 - val_loss: 17.3697\n",
      "Epoch 322/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5570 - val_loss: 17.5699\n",
      "Epoch 323/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.5478 - val_loss: 17.6842\n",
      "Epoch 324/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4666 - val_loss: 17.7056\n",
      "Epoch 325/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4205 - val_loss: 17.6375\n",
      "Epoch 326/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4880 - val_loss: 17.4160\n",
      "Epoch 327/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.4478 - val_loss: 18.0035\n",
      "Epoch 328/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3929 - val_loss: 17.3317\n",
      "Epoch 329/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4162 - val_loss: 17.4670\n",
      "Epoch 330/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2850 - val_loss: 18.3047\n",
      "Epoch 331/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4199 - val_loss: 17.5277\n",
      "Epoch 332/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3816 - val_loss: 18.0662\n",
      "Epoch 333/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.4037 - val_loss: 17.5119\n",
      "Epoch 334/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2076 - val_loss: 17.4120\n",
      "Epoch 335/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3055 - val_loss: 17.8929\n",
      "Epoch 336/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2104 - val_loss: 17.3658\n",
      "Epoch 337/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.3382 - val_loss: 17.4570\n",
      "Epoch 338/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2257 - val_loss: 17.7865\n",
      "Epoch 339/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.4246 - val_loss: 17.3343\n",
      "Epoch 340/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2724 - val_loss: 17.8739\n",
      "Epoch 341/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2809 - val_loss: 17.9383\n",
      "Epoch 342/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2747 - val_loss: 17.6723\n",
      "Epoch 343/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0047 - val_loss: 17.5542\n",
      "Epoch 344/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2956 - val_loss: 18.2904\n",
      "Epoch 345/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.1334 - val_loss: 17.3686\n",
      "Epoch 346/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.3062 - val_loss: 17.6831\n",
      "Epoch 347/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.1113 - val_loss: 17.6029\n",
      "Epoch 348/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.1163 - val_loss: 17.6762\n",
      "Epoch 349/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0776 - val_loss: 17.8787\n",
      "Epoch 350/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.4395 - val_loss: 17.6984\n",
      "Epoch 351/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.2533 - val_loss: 17.9923\n",
      "Epoch 352/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.1590 - val_loss: 17.6589\n",
      "Epoch 353/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.1737 - val_loss: 17.3476\n",
      "Epoch 354/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.1221 - val_loss: 17.6278\n",
      "Epoch 355/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.2020 - val_loss: 17.8021\n",
      "Epoch 356/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0066 - val_loss: 17.4509\n",
      "Epoch 357/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0566 - val_loss: 17.3025\n",
      "Epoch 358/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0986 - val_loss: 17.8263\n",
      "Epoch 359/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.1025 - val_loss: 17.4809\n",
      "Epoch 360/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 15.0008 - val_loss: 17.9954\n",
      "Epoch 361/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0366 - val_loss: 17.2769\n",
      "Epoch 362/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 15.2426 - val_loss: 17.9396\n",
      "Epoch 363/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 15.0304 - val_loss: 17.4184\n",
      "Epoch 364/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 15.2517 - val_loss: 17.4127\n",
      "Epoch 365/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 15.2052 - val_loss: 17.9724\n",
      "Epoch 366/500\n",
      "36/36 [==============================] - 0s 7ms/step - loss: 15.1277 - val_loss: 17.5636\n",
      "Epoch 367/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 15.0341 - val_loss: 17.3979\n",
      "Epoch 368/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.9951 - val_loss: 17.9354\n",
      "Epoch 369/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9425 - val_loss: 17.4069\n",
      "Epoch 370/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.1637 - val_loss: 17.6050\n",
      "Epoch 371/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9380 - val_loss: 17.6099\n",
      "Epoch 372/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8487 - val_loss: 18.1442\n",
      "Epoch 373/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0611 - val_loss: 17.4782\n",
      "Epoch 374/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9719 - val_loss: 17.5415\n",
      "Epoch 375/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8244 - val_loss: 18.2001\n",
      "Epoch 376/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8767 - val_loss: 17.2250\n",
      "Epoch 377/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0232 - val_loss: 17.1303\n",
      "Epoch 378/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0253 - val_loss: 17.2704\n",
      "Epoch 379/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 15.0093 - val_loss: 17.6738\n",
      "Epoch 380/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8991 - val_loss: 17.4497\n",
      "Epoch 381/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9178 - val_loss: 17.1675\n",
      "Epoch 382/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7910 - val_loss: 17.3368\n",
      "Epoch 383/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7313 - val_loss: 17.0735\n",
      "Epoch 384/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8824 - val_loss: 17.5818\n",
      "Epoch 385/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8214 - val_loss: 17.5555\n",
      "Epoch 386/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8531 - val_loss: 17.3340\n",
      "Epoch 387/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.8323 - val_loss: 17.5448\n",
      "Epoch 388/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 15.0240 - val_loss: 17.1935\n",
      "Epoch 389/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7234 - val_loss: 17.3945\n",
      "Epoch 390/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8119 - val_loss: 17.7405\n",
      "Epoch 391/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8172 - val_loss: 17.1428\n",
      "Epoch 392/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.9025 - val_loss: 17.2804\n",
      "Epoch 393/500\n",
      "36/36 [==============================] - 0s 8ms/step - loss: 14.8498 - val_loss: 17.2284\n",
      "Epoch 394/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8065 - val_loss: 17.4243\n",
      "Epoch 395/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8903 - val_loss: 17.3023\n",
      "Epoch 396/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.9646 - val_loss: 17.1419\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7378 - val_loss: 17.4211\n",
      "Epoch 398/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7427 - val_loss: 17.5240\n",
      "Epoch 399/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.7773 - val_loss: 17.5153\n",
      "Epoch 400/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.5726 - val_loss: 17.2292\n",
      "Epoch 401/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.8767 - val_loss: 17.7405\n",
      "Epoch 402/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.8248 - val_loss: 16.9871\n",
      "Epoch 403/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6082 - val_loss: 17.2047\n",
      "Epoch 404/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.8781 - val_loss: 17.5391\n",
      "Epoch 405/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6174 - val_loss: 17.0554\n",
      "Epoch 406/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.8725 - val_loss: 17.4368\n",
      "Epoch 407/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6719 - val_loss: 17.7283\n",
      "Epoch 408/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6051 - val_loss: 17.0141\n",
      "Epoch 409/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6181 - val_loss: 17.1342\n",
      "Epoch 410/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6713 - val_loss: 16.9322\n",
      "Epoch 411/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5815 - val_loss: 17.2645\n",
      "Epoch 412/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6352 - val_loss: 17.1514\n",
      "Epoch 413/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.7577 - val_loss: 17.2350\n",
      "Epoch 414/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4672 - val_loss: 17.3922\n",
      "Epoch 415/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5590 - val_loss: 17.2419\n",
      "Epoch 416/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6856 - val_loss: 17.0569\n",
      "Epoch 417/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6005 - val_loss: 17.0150\n",
      "Epoch 418/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5858 - val_loss: 17.4948\n",
      "Epoch 419/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.4934 - val_loss: 17.6697\n",
      "Epoch 420/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6357 - val_loss: 17.0426\n",
      "Epoch 421/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.5875 - val_loss: 17.2456\n",
      "Epoch 422/500\n",
      "36/36 [==============================] - 0s 6ms/step - loss: 14.6992 - val_loss: 17.3262\n",
      "Epoch 423/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4157 - val_loss: 17.3431\n",
      "Epoch 424/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5783 - val_loss: 17.3658\n",
      "Epoch 425/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.8373 - val_loss: 16.9286\n",
      "Epoch 426/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5959 - val_loss: 16.9423\n",
      "Epoch 427/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6255 - val_loss: 17.1718\n",
      "Epoch 428/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.8292 - val_loss: 17.9755\n",
      "Epoch 429/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.7459 - val_loss: 17.1651\n",
      "Epoch 430/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5945 - val_loss: 17.3131\n",
      "Epoch 431/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.5618 - val_loss: 17.7392\n",
      "Epoch 432/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4378 - val_loss: 16.8864\n",
      "Epoch 433/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5229 - val_loss: 17.1927\n",
      "Epoch 434/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5188 - val_loss: 16.9413\n",
      "Epoch 435/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5407 - val_loss: 17.0056\n",
      "Epoch 436/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4184 - val_loss: 17.2569\n",
      "Epoch 437/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4686 - val_loss: 17.1185\n",
      "Epoch 438/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4916 - val_loss: 17.0443\n",
      "Epoch 439/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4436 - val_loss: 17.3984\n",
      "Epoch 440/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.6641 - val_loss: 17.5012\n",
      "Epoch 441/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4103 - val_loss: 16.9806\n",
      "Epoch 442/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5686 - val_loss: 17.1483\n",
      "Epoch 443/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4498 - val_loss: 17.3617\n",
      "Epoch 444/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6894 - val_loss: 16.9983\n",
      "Epoch 445/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5348 - val_loss: 17.3877\n",
      "Epoch 446/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6025 - val_loss: 17.1996\n",
      "Epoch 447/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4270 - val_loss: 17.1871\n",
      "Epoch 448/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6312 - val_loss: 17.1809\n",
      "Epoch 449/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4653 - val_loss: 17.2548\n",
      "Epoch 450/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3995 - val_loss: 17.1071\n",
      "Epoch 451/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4029 - val_loss: 17.2153\n",
      "Epoch 452/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3108 - val_loss: 16.9600\n",
      "Epoch 453/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4737 - val_loss: 17.4935\n",
      "Epoch 454/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4826 - val_loss: 17.1423\n",
      "Epoch 455/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3369 - val_loss: 17.4491\n",
      "Epoch 456/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4669 - val_loss: 17.0702\n",
      "Epoch 457/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3677 - val_loss: 17.3014\n",
      "Epoch 458/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.5388 - val_loss: 17.3281\n",
      "Epoch 459/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3765 - val_loss: 17.0776\n",
      "Epoch 460/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4578 - val_loss: 17.3006\n",
      "Epoch 461/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.4633 - val_loss: 17.1254\n",
      "Epoch 462/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.6008 - val_loss: 17.1233\n",
      "Epoch 463/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4903 - val_loss: 17.3390\n",
      "Epoch 464/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2100 - val_loss: 17.3512\n",
      "Epoch 465/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1242 - val_loss: 16.7049\n",
      "Epoch 466/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2455 - val_loss: 16.9522\n",
      "Epoch 467/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2637 - val_loss: 17.2430\n",
      "Epoch 468/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1995 - val_loss: 17.3410\n",
      "Epoch 469/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3403 - val_loss: 16.9157\n",
      "Epoch 470/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3889 - val_loss: 17.1181\n",
      "Epoch 471/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 13.9958 - val_loss: 17.7229\n",
      "Epoch 472/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3074 - val_loss: 16.8048\n",
      "Epoch 473/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3595 - val_loss: 17.7683\n",
      "Epoch 474/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1265 - val_loss: 17.2002\n",
      "Epoch 475/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3485 - val_loss: 17.1511\n",
      "Epoch 476/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1687 - val_loss: 17.4329\n",
      "Epoch 477/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2869 - val_loss: 17.1547\n",
      "Epoch 478/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2369 - val_loss: 16.8088\n",
      "Epoch 479/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1144 - val_loss: 17.2957\n",
      "Epoch 480/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1874 - val_loss: 16.6924\n",
      "Epoch 481/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.1501 - val_loss: 17.2429\n",
      "Epoch 482/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.3445 - val_loss: 16.9422\n",
      "Epoch 483/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2134 - val_loss: 17.1335\n",
      "Epoch 484/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2163 - val_loss: 16.9398\n",
      "Epoch 485/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1659 - val_loss: 17.0656\n",
      "Epoch 486/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.4284 - val_loss: 16.7503\n",
      "Epoch 487/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.2906 - val_loss: 17.7814\n",
      "Epoch 488/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2646 - val_loss: 17.1196\n",
      "Epoch 489/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.3195 - val_loss: 17.1875\n",
      "Epoch 490/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2368 - val_loss: 17.7759\n",
      "Epoch 491/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.2187 - val_loss: 17.0371\n",
      "Epoch 492/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1766 - val_loss: 16.9862\n",
      "Epoch 493/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1447 - val_loss: 16.9429\n",
      "Epoch 494/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1749 - val_loss: 17.2906\n",
      "Epoch 495/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1740 - val_loss: 17.1644\n",
      "Epoch 496/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 13.8878 - val_loss: 16.9791\n",
      "Epoch 497/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 13.9667 - val_loss: 19.2680\n",
      "Epoch 498/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1204 - val_loss: 17.0543\n",
      "Epoch 499/500\n",
      "36/36 [==============================] - 0s 5ms/step - loss: 14.1622 - val_loss: 17.2134\n",
      "Epoch 500/500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 14.0984 - val_loss: 17.0673\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,batch_size=128,epochs=500,validation_split = 0.3,callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2170"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(X_test)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.329422900413253"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(y_test,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
